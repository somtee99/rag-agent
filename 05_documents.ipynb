{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e",
   "metadata": {
    "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3b4e8b-269c-4cc8-8470-1db4a91b6c34",
   "metadata": {
    "id": "1c3b4e8b-269c-4cc8-8470-1db4a91b6c34"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 5:** Working with Large Documents</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "In the previous notebook, we learned about running state chains and knowledge bases! By the end, we had all the tools necessary to do some simple dialog management and custom knowledge tracking. In this notebook, we will take the same ideas and move towards the space of large documents, considering what kinds of issues we will run into as we try to incorporate large files into our LLM contexts.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Get familiar with document loaders and the kinds of utilities they might provide you.\n",
    "- Learn how to parse large documents with limited context room by chunking the document and building up a knowledge base progressively.\n",
    "- Understand how the progressive recontextualization, coersion, and consolidation of document chunks can be extremely useful, and also where it will encounter natural limitations.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "- Looking at the chunks that come out of your ArxivParser, you'll notice that some of the chunks make little sense on their own or have been completely corrupted by the conversion to text. Is it doing a pass over the chunks to clean them up?\n",
    "- Considering the document summarization workflow (or any similar workflow that processes through a large list of document chunks), how often should this happen, and when is it justifiable?\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Environment Setup:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9214bd93-d65d-4dbd-94e3-254a2f670c52",
   "metadata": {
    "id": "9214bd93-d65d-4dbd-94e3-254a2f670c52"
   },
   "outputs": [],
   "source": [
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -qq langchain langchain-nvidia-ai-endpoints gradio\n",
    "# %pip install -qq arxiv pymupdf\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"NVIDIA_API_KEY\"] = \"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\"\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55c33c07-19b8-4c81-8d99-30fa2b3b2017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\okafo\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:193: UserWarning: An API key is required for the hosted NIM. This will become an error in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Model(id='mistralai/magistral-small-2506', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='writer/palmyra-med-70b-32k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b-32k'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='rakuten/rakutenai-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-ultra-253b-v1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=True, base_model=None),\n",
       " Model(id='microsoft/phi-3.5-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='microsoft/phi-3.5-moe-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/mamba-codestral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/gemma-2-2b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='openai/gpt-oss-120b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/llama3-chatqa-1.5-8b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-8b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/mistral-large-2-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='01-ai/yi-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-yi-large'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama-3.1-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='qwen/qwen2.5-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/mistral-nemo-minitron-8b-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='seallms/seallm-7b-v2.5', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-seallm-7b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='igenius/italia_10b_instruct_16k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='writer/palmyra-creative-122b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='microsoft/kosmos-2', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/kosmos-2', aliases=['ai-microsoft-kosmos-2', 'playground_kosmos_2', 'kosmos_2'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-distill-qwen-32b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='qwen/qwq-32b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=True, base_model=None),\n",
       " Model(id='nvidia/usdcode-llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama-3.2-1b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='google/gemma-2-9b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-9b-it'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/recurrentgemma-2b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-recurrentgemma-2b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/mistral-small-24b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='rakuten/rakutenai-7b-chat', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='aisingapore/sea-lion-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-sea-lion-7b-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='ibm/granite-3.3-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=True, base_model=None),\n",
       " Model(id='thudm/chatglm3-6b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama-guard-4-12b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/gemma-3-4b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/codestral-22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codestral-22b-instruct-v01'], supports_tools=False, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/mistral-7b-instruct-v0.3', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v03'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/shieldgemma-9b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama-3.3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/nemotron-4-340b-reward', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-mini-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/nemoretriever-parse', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-distill-llama-8b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/deplot', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/google/deplot', aliases=['ai-google-deplot', 'playground_deplot', 'deplot'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/gemma-2b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2b', 'playground_gemma_2b', 'gemma_2b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/vila', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/nvidia/vila', aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='microsoft/phi-3.5-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.3-nemotron-super-49b-v1.5', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=True, base_model=None),\n",
       " Model(id='mistralai/mistral-nemotron', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemoguard-8b-content-safety', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-70b-reward', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='ibm/granite-3.0-3b-a800m-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-nano-vl-8b-v1', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/usdcode-llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama-3.2-3b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-0528', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama-3.2-11b-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions', aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='abacusai/dracarys-llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/nemotron-4-mini-hindi-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/riva-translate-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='igenius/colosseum_355b_instruct_16k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama2-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama2-70b', 'playground_llama2_70b', 'llama2_70b', 'playground_llama2_13b', 'llama2_13b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nv-mistralai/mistral-nemo-12b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-51b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='tokyotech-llm/llama-3-swallow-70b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-distill-qwen-14b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='databricks/dbrx-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-dbrx-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='upstage/solar-10.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-solar-10_7b-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='baichuan-inc/baichuan2-13b-chat', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='gotocompany/gemma-2-9b-cpt-sahabatai-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-70b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-nano-4b-v1.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=True, base_model=None),\n",
       " Model(id='microsoft/phi-4-multimodal-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='utter-project/eurollm-9b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama-4-scout-17b-16e-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama-3.1-405b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='openai/gpt-oss-20b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/codellama-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codellama-70b', 'playground_llama2_code_70b', 'llama2_code_70b', 'playground_llama2_code_34b', 'llama2_code_34b', 'playground_llama2_code_13b', 'llama2_code_13b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='zyphra/zamba2-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/mistral-7b-instruct-v0.2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v2', 'playground_mistral_7b', 'mistral_7b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/mistral-small-3.1-24b-instruct-2503', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/nemotron-4-340b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['qa-nemotron-4-340b-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvdev/meta/llama-4-maverick-17b-128e-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='writer/palmyra-med-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/gemma-2-27b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-27b-it'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='microsoft/phi-4-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/mistral-medium-3-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='ibm/granite-guardian-3.0-8b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/nemotron-mini-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='adept/fuyu-8b', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/adept/fuyu-8b', aliases=['ai-fuyu-8b', 'playground_fuyu_8b', 'fuyu_8b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/llama3-chatqa-1.5-70b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-70b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/nvclip', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='writer/palmyra-fin-70b-32k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-nano-8b-v1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=True, base_model=None),\n",
       " Model(id='mediatek/breeze-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-breeze-7b-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='qwen/qwen2.5-coder-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-distill-qwen-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='qwen/qwen2-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='marin/marin-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/mixtral-8x22b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-medium-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-128k-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/gemma-3-27b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='ibm/granite-8b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-8b-code-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama3-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-8b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-medium-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-4k-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-small-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-128k-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama-3.2-90b-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-90b-vision-instruct/chat/completions', aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='ai21labs/jamba-1.5-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/codegemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-7b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='meta/llama-4-maverick-17b-128e-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-mini-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini-4k', 'playground_phi2', 'phi2'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='ai21labs/jamba-1.5-large-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-coder-6.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-deepseek-coder-6_7b-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='speakleash/bielik-11b-v2.3-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='qwen/qwen3-235b-a22b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=True, base_model=None),\n",
       " Model(id='microsoft/phi-3-vision-128k-instruct', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/phi-3-vision-128k-instruct', aliases=['ai-phi-3-vision-128k-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='snowflake/arctic', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-arctic'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/gemma-3n-e4b-it', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/mistral-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-large'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='ibm/granite-34b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-34b-code-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/gemma-3-1b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='tiiuae/falcon3-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/paligemma', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/google/paligemma', aliases=['ai-google-paligemma'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='yentinglin/llama-3-taiwan-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='qwen/qwen2.5-coder-32b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.3-nemotron-super-49b-v1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=True, base_model=None),\n",
       " Model(id='microsoft/phi-4-mini-flash-reasoning', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/codegemma-1.1-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-1.1-7b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/mixtral-8x22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x22b-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='ibm/granite-3.0-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/neva-22b', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b', aliases=['ai-neva-22b', 'playground_neva_22b', 'neva_22b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/gemma-3n-e2b-it', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemoguard-8b-topic-control', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/gemma-3-12b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-small-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-8k-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/mixtral-8x7b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x7b-instruct', 'playground_mixtral_8x7b', 'mixtral_8x7b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='google/gemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-7b', 'playground_gemma_7b', 'gemma_7b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None),\n",
       " Model(id='moonshotai/kimi-k2-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, base_model=None),\n",
       " Model(id='mistralai/mathstral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, base_model=None)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "ChatNVIDIA.get_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de8583a1-c10a-41da-8256-49520f868670",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful utility method for printing intermediate states\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from functools import partial\n",
    "\n",
    "def RPrint(preface=\"State: \"):\n",
    "    def print_and_return(x, preface=\"\"):\n",
    "        print(f\"{preface}{x}\")\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def PPrint(preface=\"State: \"):\n",
    "    def print_and_return(x, preface=\"\"):\n",
    "        pprint(preface, x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 1:** Chatting with Documents\n",
    "\n",
    "This notebook will begin a longer stream of discussion surrounding the use of LLMs to chat with documents. In a world where chat models are trained on giant repositories of public data and retraining them on custom data is prohibitively expensive, the idea of having an LLM reason about a set of PDFs or even a YouTube video opens up many opportunities!\n",
    "\n",
    "- **Your LLM can have a modifiable knowledge base grounded in human-readible documents,** meaning that you can directly control what kinds of data it has access to and can instruct it to interact with it.\n",
    "\n",
    "- **Your LLM can sort through and pull references directly from your document set.** With sufficient prompt engineering and instruction-following priors, you can force your models to only act based on the material you provide.\n",
    "\n",
    "- **Your LLM can possibly even interact with your documents, making automatic modifications as necessary.** This opens up avenues in automatic content refinement and synthetic operations which will be explored later.\n",
    "\n",
    "Listing out some possibilities is pretty easy, and from there you can let your imagination run wild... but we haven't gained the tools to do this quite yet, right?\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Naive Approach: Stuff Your Document**\n",
    "\n",
    "Suppose you have some text documents (PDF, blog, etc.) and want to ask questions related to the contents of those documents. One approach you could try involves taking a representation of the document and feeding it all to a chat model! From a document perspective, this is known as [**document stuffing**](https://js.langchain.com/v0.1/docs/modules/chains/document/stuff/).\n",
    "\n",
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=14DRI_uDviqzqg14TKoIc8IlBc3Zsb8oO\" width=800px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/doc_stuff.png\" width=800px/>\n",
    ">\n",
    "> From [**Stuff | LangChain**ü¶úÔ∏èüîó](https://js.langchain.com/v0.1/docs/modules/chains/document/stuff/)\n",
    "\n",
    "<br>\n",
    "\n",
    "This may very well work if your model is strong enough and if your document is short enough, but it shouldn't be expected to work well for an entire document. Many modern LLMs have significant trouble working with long contexts due to training limitations. Nowadays large model deterioration isn't quite as catastrophic, but good instruction following is likely to fall apart pretty quickly regardless of which model you use (assuming you're accessing the raw model).\n",
    "\n",
    "<br>\n",
    "\n",
    "**The key issues you'll need to resolve with document reasoning are:**\n",
    "\n",
    "- How do we split our documents into pieces that can be reasoned with?\n",
    "\n",
    "- How can we find and consider these pieces efficiently as the size and number of documents increases?\n",
    "\n",
    "This course will explore several approaches to address these issues while continuing to build up our LLM orchestration skills. ***This notebook will serve to expand out our previous running chain skills for more progressive reasoning formulations, whereas the next notebooks will introduce some new techniques to properly address retrieval at scale.*** Through this experience, we will continue to leverage cutting-edge open-source solutions to make our solutions standard and integratable.\n",
    "\n",
    "Speaking of, the field of document loading frameworks has many strong options, and two major players will come up throughout the course:\n",
    "\n",
    "- [**LangChain**](https://python.langchain.com/docs/get_started/introduction) provides a simple framework for connecting LLMs to your own data sources via general chunking strategies and strong incorporation with embedding frameworks/services. This framework has initially grown around its strong general support for LLM features, which signals its active strengths closer to the chain abstractions and agent coordination.\n",
    "\n",
    "- [**LlamaIndex**](https://gpt-index.readthedocs.io/en/stable/) is a data framework for LLM applications to ingest, structure, and access private or domain-specific data. It has since branched out to include general LLM capabilities similar to LangChain, but as of now it is still strongest in addressing the document side of LLM components since its initial abstractions were centered around that problem.\n",
    "\n",
    "It's recommended to read more about the unique strengths of both LlamaIndex and LangChain and pick the one that works best for you. Since LlamaIndex can be used *with* LangChain, the frameworks' unique capabilities can be leveraged together without too much issue. For the sake of simplicity, we will stick to LangChain in this course and will allow the [**NVIDIA/GenerativeAIExamples repository**](https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RAG/notebooks) to explore deeper LlamaIndex options for those interested.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310462b-f215-4d00-9d59-e613921bed0a",
   "metadata": {
    "id": "3310462b-f215-4d00-9d59-e613921bed0a"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** Loading Documents\n",
    "\n",
    "LangChain provides a variety of [document loaders](https://python.langchain.com/docs/integrations/document_loaders) to facilitate the injestion of various document formats (HTML, PDF, code) from many different sources and locations (local storage, private s3 buckets, public websites, messaging APIs, etc.). These loaders query your data sources and return a `Document` object which contains the content and metadata, usually in a plain-text or otherwise human-readible format. There are plenty of document loaders already built and ready to use, with the first-party LangChain options listed [here](https://python.langchain.com/docs/integrations/document_loaders).\n",
    "\n",
    "**In this example, we can load a research paper of our choice using one of the following LangChain loaders:**\n",
    "- [`UnstructuredFileLoader`](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file): Generally-useful file loader for arbitrary files; doesn't make too many assumptions about your document structure and is usually sufficient.\n",
    "- [`ArxivLoader`](https://python.langchain.com/docs/integrations/document_loaders/arxiv): A more specialized file-loader which can communicate with the Arxiv interface directly. [Just one example of many](https://python.langchain.com/docs/integrations/document_loaders), this will make some more assumptions about your data to yield nicer parsings and auto-fill metadata (useful when you have multiple documents/formats).\n",
    "\n",
    "For our code example we will default to using `ArxivLoader` to load in one of either the [MRKL](https://arxiv.org/abs/2205.00445) or [ReAct](https://arxiv.org/abs/2210.03629) publication papers as you're likely to run into them at some point in your continued chat model research endeavors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "834edbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.18.0 requires tensorboard<2.19,>=2.18, which is not installed.\n",
      "alpaca-trade-api 3.2.0 requires websockets<11,>=9.0, but you have websockets 15.0.1 which is incompatible.\n",
      "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.5 which is incompatible.\n",
      "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\n",
      "langchain-nvidia-ai-endpoints 0.3.18 requires langchain-core<0.4,>=0.3.51, but you have langchain-core 1.0.5 which is incompatible.\n",
      "langserve 0.3.2 requires langchain-core<0.4,>=0.3, but you have langchain-core 1.0.5 which is incompatible.\n",
      "tensorflow-intel 2.18.0 requires ml-dtypes<0.5.0,>=0.4.0, but you have ml-dtypes 0.5.3 which is incompatible.\n",
      "conda-repo-cli 1.0.114 requires urllib3>=2.2.2, but you have urllib3 1.26.20 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.0.1 (from langchain-community)\n",
      "  Downloading langchain_core-1.0.5-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
      "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (2.0.34)\n",
      "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (8.2.3)\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (0.4.29)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
      "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.8.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (24.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (4.11.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.27.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.21.0)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain-community)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.20.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\n",
      "Collecting typing-extensions<5.0.0,>=4.7.0 (from langchain-core<2.0.0,>=1.0.1->langchain-community)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.5/2.5 MB 16.2 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 48.7 MB/s eta 0:00:00\n",
      "Downloading langchain_core-1.0.5-py3-none-any.whl (471 kB)\n",
      "Downloading pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: typing-extensions, requests, marshmallow, httpx-sse, typing-inspection, typing-inspect, dataclasses-json, pydantic-settings, langchain-core, langchain-text-splitters, langchain-classic, langchain-community\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.76\n",
      "    Uninstalling langchain-core-0.3.76:\n",
      "      Successfully uninstalled langchain-core-0.3.76\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.3.11\n",
      "    Uninstalling langchain-text-splitters-0.3.11:\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.11\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.3 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-core-1.0.5 langchain-text-splitters-1.0.0 marshmallow-3.26.1 pydantic-settings-2.12.0 requests-2.32.5 typing-extensions-4.15.0 typing-inspect-0.9.0 typing-inspection-0.4.2\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15a4afb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting arxiv\n",
      "  Downloading arxiv-2.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: requests~=2.32.0 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from arxiv) (2.32.5)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from requests~=2.32.0->arxiv) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests~=2.32.0->arxiv) (2024.8.30)\n",
      "Downloading arxiv-2.3.1-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6060 sha256=0445fb43cde579e5517d20e154937a3ea2cd95eb082df9a788869fbd8483c2fe\n",
      "  Stored in directory: c:\\users\\okafo\\appdata\\local\\pip\\cache\\wheels\\03\\f5\\1a\\23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "Successfully installed arxiv-2.3.1 feedparser-6.0.12 sgmllib3k-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "374efc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.26.6-cp310-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.26.6-cp310-abi3-win_amd64.whl (18.4 MB)\n",
      "   ---------------------------------------- 0.0/18.4 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 6.8/18.4 MB 42.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.4/18.4 MB 52.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.4/18.4 MB 48.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.26.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca09418b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting unstructured\n",
      "  Downloading unstructured-0.18.20-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: charset-normalizer in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured) (3.3.2)\n",
      "Requirement already satisfied: filetype in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (1.2.0)\n",
      "Collecting python-magic (from unstructured)\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: lxml in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured) (5.2.1)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured) (3.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured) (4.12.3)\n",
      "Collecting emoji (from unstructured)\n",
      "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (0.6.7)\n",
      "Collecting python-iso639 (from unstructured)\n",
      "  Downloading python_iso639-2025.11.16-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting langdetect (from unstructured)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 981.5/981.5 kB 15.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured) (1.26.4)\n",
      "Collecting rapidfuzz (from unstructured)\n",
      "  Downloading rapidfuzz-3.14.3-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Collecting backoff (from unstructured)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured) (4.15.0)\n",
      "Collecting unstructured-client (from unstructured)\n",
      "  Downloading unstructured_client-0.42.4-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: wrapt in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured) (1.14.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured) (4.66.5)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured) (5.9.0)\n",
      "Collecting python-oxmsg (from unstructured)\n",
      "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting html5lib (from unstructured)\n",
      "  Using cached html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->unstructured) (2.5)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json->unstructured) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from html5lib->unstructured) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\programdata\\anaconda3\\lib\\site-packages (from html5lib->unstructured) (0.5.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->unstructured) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->unstructured) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->unstructured) (2024.9.11)\n",
      "Collecting olefile (from python-oxmsg->unstructured)\n",
      "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->unstructured) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from requests->unstructured) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->unstructured) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->unstructured) (0.4.6)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured-client->unstructured) (24.1.0)\n",
      "Requirement already satisfied: cryptography>=3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured-client->unstructured) (43.0.0)\n",
      "Collecting httpcore>=1.0.9 (from unstructured-client->unstructured)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured-client->unstructured) (0.27.0)\n",
      "Collecting pydantic>=2.11.2 (from unstructured-client->unstructured)\n",
      "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
      "Collecting pypdf>=6.2.0 (from unstructured-client->unstructured)\n",
      "  Downloading pypdf-6.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
      "Collecting h11>=0.16 (from httpcore>=1.0.9->unstructured-client->unstructured)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.11.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (24.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.6.0)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic>=2.11.2->unstructured-client->unstructured)\n",
      "  Downloading pydantic_core-2.41.5-cp312-cp312-win_amd64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.4.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.21)\n",
      "Downloading unstructured-0.18.20-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 24.5 MB/s eta 0:00:00\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
      "   ---------------------------------------- 0.0/608.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 608.4/608.4 kB 21.6 MB/s eta 0:00:00\n",
      "Using cached html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Downloading python_iso639-2025.11.16-py3-none-any.whl (167 kB)\n",
      "Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
      "Downloading rapidfuzz-3.14.3-cp312-cp312-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 85.1 MB/s eta 0:00:00\n",
      "Downloading unstructured_client-0.42.4-py3-none-any.whl (207 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 37.4 MB/s eta 0:00:00\n",
      "Downloading pypdf-6.3.0-py3-none-any.whl (328 kB)\n",
      "Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993251 sha256=f6c356cab55ec9acb821c2425adaec1d0125b56932bf9021a09a5a063b70539f\n",
      "  Stored in directory: c:\\users\\okafo\\appdata\\local\\pip\\cache\\wheels\\c1\\67\\88\\e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
      "Successfully built langdetect\n",
      "Installing collected packages: rapidfuzz, python-magic, python-iso639, pypdf, pydantic-core, olefile, langdetect, html5lib, h11, emoji, backoff, python-oxmsg, pydantic, httpcore, unstructured-client, unstructured\n",
      "Successfully installed backoff-2.2.1 emoji-2.15.0 h11-0.16.0 html5lib-1.1 httpcore-1.0.9 langdetect-1.0.9 olefile-0.47 pydantic-2.12.4 pydantic-core-2.41.5 pypdf-6.3.0 python-iso639-2025.11.16 python-magic-0.4.27 python-oxmsg-0.0.2 rapidfuzz-3.14.3 unstructured-0.18.20 unstructured-client-0.42.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradio 5.46.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
      "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.5 which is incompatible.\n",
      "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\n",
      "langchain-nvidia-ai-endpoints 0.3.18 requires langchain-core<0.4,>=0.3.51, but you have langchain-core 1.0.5 which is incompatible.\n",
      "langserve 0.3.2 requires langchain-core<0.4,>=0.3, but you have langchain-core 1.0.5 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c091cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: unstructured[docx] in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (0.18.20)\n",
      "Requirement already satisfied: charset-normalizer in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured[docx]) (3.3.2)\n",
      "Requirement already satisfied: filetype in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured[docx]) (1.2.0)\n",
      "Requirement already satisfied: python-magic in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured[docx]) (0.4.27)\n",
      "Requirement already satisfied: lxml in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured[docx]) (5.2.1)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured[docx]) (3.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured[docx]) (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured[docx]) (4.12.3)\n",
      "Requirement already satisfied: emoji in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured[docx]) (2.15.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured[docx]) (0.6.7)\n",
      "Requirement already satisfied: python-iso639 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured[docx]) (2025.11.16)\n",
      "Requirement already satisfied: langdetect in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured[docx]) (1.0.9)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured[docx]) (1.26.4)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured[docx]) (3.14.3)\n",
      "Requirement already satisfied: backoff in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured[docx]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured[docx]) (4.15.0)\n",
      "Requirement already satisfied: unstructured-client in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured[docx]) (0.42.4)\n",
      "Requirement already satisfied: wrapt in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured[docx]) (1.14.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured[docx]) (4.66.5)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured[docx]) (5.9.0)\n",
      "Requirement already satisfied: python-oxmsg in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured[docx]) (0.0.2)\n",
      "Requirement already satisfied: html5lib in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured[docx]) (1.1)\n",
      "Collecting python-docx>=1.1.2 (from unstructured[docx])\n",
      "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->unstructured[docx]) (2.5)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json->unstructured[docx]) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json->unstructured[docx]) (0.9.0)\n",
      "Requirement already satisfied: six>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from html5lib->unstructured[docx]) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\programdata\\anaconda3\\lib\\site-packages (from html5lib->unstructured[docx]) (0.5.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->unstructured[docx]) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->unstructured[docx]) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->unstructured[docx]) (2024.9.11)\n",
      "Requirement already satisfied: olefile in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from python-oxmsg->unstructured[docx]) (0.47)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->unstructured[docx]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from requests->unstructured[docx]) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->unstructured[docx]) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->unstructured[docx]) (0.4.6)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured-client->unstructured[docx]) (24.1.0)\n",
      "Requirement already satisfied: cryptography>=3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured-client->unstructured[docx]) (43.0.0)\n",
      "Requirement already satisfied: httpcore>=1.0.9 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured-client->unstructured[docx]) (1.0.9)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured-client->unstructured[docx]) (0.27.0)\n",
      "Requirement already satisfied: pydantic>=2.11.2 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured-client->unstructured[docx]) (2.12.4)\n",
      "Requirement already satisfied: pypdf>=6.2.0 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from unstructured-client->unstructured[docx]) (6.3.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from unstructured-client->unstructured[docx]) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from cryptography>=3.1->unstructured-client->unstructured[docx]) (1.17.1)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from httpcore>=1.0.9->unstructured-client->unstructured[docx]) (0.16.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured[docx]) (4.11.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured[docx]) (1.3.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured[docx]) (24.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client->unstructured[docx]) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from pydantic>=2.11.2->unstructured-client->unstructured[docx]) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\okafo\\appdata\\roaming\\python\\python312\\site-packages (from pydantic>=2.11.2->unstructured-client->unstructured[docx]) (0.4.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[docx]) (1.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured[docx]) (2.21)\n",
      "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install unstructured[docx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4382b61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3944,
     "status": "ok",
     "timestamp": 1703112979370,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "b4382b61",
    "outputId": "d6e95b9b-97be-4984-a9fd-58a528091146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.22 s\n",
      "Wall time: 4.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "## Loading in the file\n",
    "\n",
    "## Unstructured File Loader: Good for arbitrary \"probably good enough\" loader\n",
    "documents = UnstructuredFileLoader(\"Multi-Type Question Sequencing Final Report.docx\").load()\n",
    "\n",
    "## More specialized loader, won't work for everything, but simple API and usually better results\n",
    "# documents = ArxivLoader(query=\"2404.16130\").load()  ## GraphRAG\n",
    "# documents = ArxivLoader(query=\"2404.03622\").load()  ## Visualization-of-Thought\n",
    "# documents = ArxivLoader(query=\"2404.19756\").load()  ## KAN: Kolmogorov-Arnold Networks\n",
    "# documents = ArxivLoader(query=\"2404.07143\").load()  ## Infini-Attention\n",
    "# documents = ArxivLoader(query=\"2210.03629\").load()  ## ReAct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hw0SL--6cirp",
   "metadata": {
    "id": "hw0SL--6cirp"
   },
   "source": [
    "<br>\n",
    "\n",
    "We can see from our import that we this connector gives us access to two different components:\n",
    "- The `page_content` is the actual body of the document in some human-interpretable format.\n",
    "- The `metadata` is relevant information about the document that is provided by the connector via its data source.\n",
    "\n",
    "Below, we can check out the length of our document body to see what's inside, and will probably notice an intractable document length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2289d525-2c2b-4a99-9a48-00f9b951ae02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 370,
     "status": "ok",
     "timestamp": 1703113455184,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "2289d525-2c2b-4a99-9a48-00f9b951ae02",
    "outputId": "98b9ef68-c36b-478f-9bbb-1e45b2c49d60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents Retrieved: 1\n",
      "Sample of Document 1 Content (Total Length: 85651):\n",
      "1\n",
      "\n",
      "Curriculum-Aligned Adaptive Learning via Multi-Type Question Sequencing\n",
      "\n",
      "Submitted by \n",
      "\n",
      "Somtoochukwu Oliseemeka Okafor (20047643)\n",
      "\n",
      "Applied Research Project (B9AI107) submitted in partial\n",
      "fulfilment of the requirements for the degree of Master of\n",
      "Artificial Intelligence at Dublin Business School\n",
      "\n",
      "Supervised by\n",
      "Dr. Anesu Nyabadza\n",
      "\n",
      "September 2025\n",
      "\n",
      "DECLARATION\n",
      "\n",
      "I hereby declare that the Applied Research Project entitled: \"Curriculum-Aligned Adaptive Learning via Multi-Type Question Sequencing\", submitted to Dublin Business School in partial fulfilment of the requirements for the award of Master of Artificial Intelligence, is my own original work. This research has been conducted independently and without unauthorized assistance, except where explicit acknowledgement has been made through appropriate referencing.\n",
      "\n",
      "Furthermore, I confirm that this work has not previously been submitted, in whole or in part, for the purpose of obtaining any other academic degree or qualification at this or any other institution of higher education.\n",
      "\n",
      "Name: Somtoochukwu Oliseemeka Okafor.\n",
      "\n",
      "Student Number: 20047643.\n",
      "\n",
      "Signed: \n",
      "\n",
      "Date: 26th August 2025.\n",
      "\n",
      "ACKNOWLEDGEMENT\n",
      "\n",
      "I wish to convey my profound appreciation to my supervisor, Dr. Anesu Nyabadza, for their exceptional mentorship, guidance, and analytical feedback throughout the duration of this project. Their specialized knowledge and perceptive recommendations were essential in directing the scope and excellence of this research.\n",
      "\n",
      "I extend my recognition to the institutions and entities that graciously provided their datasets for public use, facilitating advancement and discovery in the domain of adaptive educational technology.\n",
      "\n",
      "My sincere appreciation is directed to Dublin Business School for supplying the essential infrastructure, materials, and academic setting that enabled the effective execution of this investigation.\n",
      "\n",
      "I hold deep respect for the dynamic open-source ecosystem, whose cutting-edge resources served a fundamental function in the creation and implementation of the system architecture.\n",
      "\n",
      "Lastly, I recognize the contribution of AI-powered resources in polishing specific portions of this document. While all research findings, analysis, and conclusions represent exclusively my original work, these technological aids provided meaningful support in the refinement process.\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "This study investigates the effectiveness of multi-type question sequencing in adaptive learning environments by developing and simulating an intelligent tutoring system. The research addresses the need for personalized educational strategies that optimize learning by selecting question types or formats based on student performance.\n",
      "\n",
      "A simulated intelligent tutoring environment was built to evaluate the impact of different question formats on learning. The system models student behaviour using the ASSISTMENTS 2009 dataset and employs Deep Knowledge Tracing (DKT) to predict student performance. This model simulates realistic student responses across various question formats and skill levels.\n",
      "\n",
      "The research features a dual-agent architecture with a student agent and a teacher agent. The DKT-powered student agent generates authentic performance predictions while the teacher agent, using Proximal Policy Optimization (PPO), dynamically adapts the learning experience by selecting and sequencing questions to enhance student engagement and learning outcomes.\n",
      "\n",
      "Key findings indicate that question types alone do not significantly influence the determination of student knowledge. However, adaptive sequencing that incorporates multiple question formats can substantially improve learning outcomes. The student agent achieved an 8% higher running average performance in stable regions when using an adaptive policy that selected both skill and question type. This policy obtained a final reward of 109.99, outperforming both the policy that selected only skills (96.34) and the random baseline (67.01). This demonstrates the system's ability to identify optimal question types based on student profiles, leading to better knowledge retention and performance. This research provides more empirical evidence for the effectiveness of AI-driven adaptive learning and offers a framework for intelligent question sequencing in digital learning environments based on content format. The study's implications are relevant for educational institutions implementing personalized learning and for researchers developing advanced adaptive tutoring systems.\n",
      "\n",
      "\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "Table of Figures\n",
      "\n",
      "\n",
      "\n",
      "Table of Tables\n",
      "\n",
      "Chapter 1\n",
      "\n",
      "Introduction\n",
      "\n",
      "\n",
      "\n",
      "1.1 Background \n",
      "\n",
      "The increasing integration of Artificial Intelligence (AI) in education has created a lot of new opportunities and possibilities for learning and teaching. Within this landscape, the combination of Natural Language Processing (NLP) and Reinforcement Learning (RL) shows great promise for developing dynamic educational systems. NLP can automatically generate diverse and contextually \n"
     ]
    }
   ],
   "source": [
    "## Printing out a sample of the content\n",
    "print(\"Number of Documents Retrieved:\", len(documents))\n",
    "print(f\"Sample of Document 1 Content (Total Length: {len(documents[0].page_content)}):\")\n",
    "print(documents[0].page_content[:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1JjUK2ZSd0HL",
   "metadata": {
    "id": "1JjUK2ZSd0HL"
   },
   "source": [
    "<br>\n",
    "\n",
    "In contrast, the metadata will be much more conservatively-sized to the point of being viable context components for your favorite chat model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "Py2lbRXlcX81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1703112982386,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "Py2lbRXlcX81",
    "outputId": "07197dd4-1609-4ecf-ae54-cf6ef3d25458"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Multi-Type Question Sequencing Final Report.docx'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[32m'source'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Multi-Type Question Sequencing Final Report.docx'\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7046ea74-0b81-400e-8364-449f421d2add",
   "metadata": {
    "id": "7046ea74-0b81-400e-8364-449f421d2add"
   },
   "source": [
    "<br>\n",
    "\n",
    "Though it may be tempting to accept the metadata format as-is and ignore the body entirely, there are a key selection of features that cannot be approached without diving into the full text:\n",
    "\n",
    "- **The metadata is not guaranteed.** In the case of `arxiv`, paper abstracts, titles, authors, and date are necessary components of a submission, so being able to query them is not surprising. For an arbitrary PDF or webpage though, the same is not necessarily the case.\n",
    "- **The agent will not be able to go deeper into the document content.** The summary is good to know and can be used as-is, but does not provide a straight-forward path to interacting with the body at any capacity (at least not from what we've learned).\n",
    "- **The agent will still not be able to reason about too many documents at once.** Perhaps in the MRKL/ReAct example, you could combine those two summaries into one context and ask some questions. But what happens when you need to interact with 5 documents at once? What about an entire directory? Very soon, you will notice that your context window will be overloaded with information just to summarize or even list out the documents you're interested in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0449e4",
   "metadata": {
    "id": "4e0449e4"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3:** Transforming The Documents\n",
    "\n",
    "Once documents have been loaded, they often need to be transformed if we intend to pass them into our LLMs as context. One method of transformation is known as **chunking**, which breaks down large pieces of content into smaller segments. This technique is valuable because it helps [optimize the relevance of the content returned from the vector database](https://www.pinecone.io/learn/chunking-strategies/).\n",
    "\n",
    "LangChain provides a [variety of document transformers](https://python.langchain.com/docs/integrations/document_transformers/) out of which we will use the [``RecursiveCharacterTextSplitter``](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter). This option will allow us to split our document based on a preference of natural stopping points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f564ee4-262e-4721-bf6b-ee8ebdb7a1ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1703112527056,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "6f564ee4-262e-4721-bf6b-ee8ebdb7a1ba",
    "outputId": "a4e666e5-5a5c-413b-f5a4-acca742d80d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "## Some nice custom preprocessing\n",
    "documents[0].page_content = documents[0].page_content.replace(\". .\", \"\")\n",
    "docs_split = text_splitter.split_documents(documents)\n",
    "\n",
    "def include_doc(doc):\n",
    "    ## Some chunks will be overburdened with useless numerical data, so we'll filter it out\n",
    "    string = doc.page_content\n",
    "    if len([l for l in string if l.isalpha()]) < (len(string)//2):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "docs_split = [doc for doc in docs_split if include_doc(doc)]\n",
    "print(len(docs_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f8bcc89-c781-44d0-9ec1-1fe45eec8b46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1703112530925,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "1f8bcc89-c781-44d0-9ec1-1fe45eec8b46",
    "outputId": "1cf24605-65bb-40a2-e7aa-e2d9a8fb6382"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Curriculum-Aligned Adaptive Learning via Multi-Type Question Sequencing\n",
      "\n",
      "Submitted by \n",
      "\n",
      "Somtoochukwu Oliseemeka Okafor (20047643)\n",
      "\n",
      "Applied Research Project (B9AI107) submitted in partial\n",
      "fulfilment of the requirements for the degree of Master of\n",
      "Artificial Intelligence at Dublin Business School\n",
      "\n",
      "Supervised by\n",
      "Dr. Anesu Nyabadza\n",
      "\n",
      "September 2025\n",
      "\n",
      "DECLARATION\n",
      "\n",
      "I hereby declare that the Applied Research Project entitled: \"Curriculum-Aligned Adaptive Learning via Multi-Type Question Sequencing\", submitted to Dublin Business School in partial fulfilment of the requirements for the award of Master of Artificial Intelligence, is my own original work. This research has been conducted independently and without unauthorized assistance, except where explicit acknowledgement has been made through appropriate referencing.\n",
      "\n",
      "Furthermore, I confirm that this work has not previously been submitted, in whole or in part, for the purpose of obtaining any other academic degree or qualification at this or any other institution of higher education.\n",
      "\n",
      "Name: Somtoochukwu Oliseemeka Okafor.\n",
      "\n",
      "Student Number: 20047643.\n",
      "\n",
      "Signed: \n",
      "\n",
      "Date: 26th August 2025.\n",
      "\n",
      "ACKNOWLEDGEMENT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Number: 20047643.\n",
      "\n",
      "Signed: \n",
      "\n",
      "Date: 26th August 2025.\n",
      "\n",
      "ACKNOWLEDGEMENT\n",
      "\n",
      "I wish to convey my profound appreciation to my supervisor, Dr. Anesu Nyabadza, for their exceptional mentorship, guidance, and analytical feedback throughout the duration of this project. Their specialized knowledge and perceptive recommendations were essential in directing the scope and excellence of this research.\n",
      "\n",
      "I extend my recognition to the institutions and entities that graciously provided their datasets for public use, facilitating advancement and discovery in the domain of adaptive educational technology.\n",
      "\n",
      "My sincere appreciation is directed to Dublin Business School for supplying the essential infrastructure, materials, and academic setting that enabled the effective execution of this investigation.\n",
      "\n",
      "I hold deep respect for the dynamic open-source ecosystem, whose cutting-edge resources served a fundamental function in the creation and implementation of the system architecture.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lastly, I recognize the contribution of AI-powered resources in polishing specific portions of this document. While all research findings, analysis, and conclusions represent exclusively my original work, these technological aids provided meaningful support in the refinement process.\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "This study investigates the effectiveness of multi-type question sequencing in adaptive learning environments by developing and simulating an intelligent tutoring system. The research addresses the need for personalized educational strategies that optimize learning by selecting question types or formats based on student performance.\n",
      "\n",
      "A simulated intelligent tutoring environment was built to evaluate the impact of different question formats on learning. The system models student behaviour using the ASSISTMENTS 2009 dataset and employs Deep Knowledge Tracing (DKT) to predict student performance. This model simulates realistic student responses across various question formats and skill levels.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 2.3: Conventional adaptive learning system .\n",
      "\n",
      "2.2 Student Modelling\n",
      "\n",
      "Significant research efforts have been directed toward enhancing student learning through Knowledge Tracing (KT) which is the process of modelling a student‚Äôs evolving understanding of concepts over time. Traditional approaches such as Bayesian Knowledge Tracing (BKT) use probabilistic models based on Hidden Markov models to estimate a student‚Äôs mastery of predefined skills . More recent methods include DKT methods that leverage Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, to capture complex temporal dependencies in students‚Äô learning trajectories . With the importance of learning duration and the increasing length of interaction sequences in adaptive learning environments, recent studies have compared LSTM-based knowledge tracing model variants with earlier methods‚Äô variants, such as Item Cognitive Structure-KT (ICS-KT) and Self-Supervised Attentive Knowledge Tracing (SSAKT), showing that these newer approaches can achieve superior predictive performance over longer question interaction sequences .\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m-1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As adaptive systems become more powerful, addressing their ethical implications is paramount. Future research must focus on algorithmic fairness and bias mitigation to ensure equitable outcomes for all students. Developing privacy preserving techniques, such as federated learning, will be critical for protecting sensitive student data. Additionally, improving the transparency and explainability of the RL agent‚Äôs decisions is necessary to foster trust among educators, students, and parents. Finally, future work should focus on integrating these systems into the broader educational ecosystem, ensuring alignment with school curricula and providing professional development to help teachers leverage these powerful new tools effectively.\n",
      "\n",
      "\n",
      "\n",
      "References\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in (0, 1, 2, 15, -1):\n",
    "    pprint(f\"[Document {i}]\")\n",
    "    print(docs_split[i].page_content)\n",
    "    pprint(\"=\"*64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e2f969-72cd-4d0e-a150-e3efafc1cdfc",
   "metadata": {
    "id": "57e2f969-72cd-4d0e-a150-e3efafc1cdfc"
   },
   "source": [
    "<br>\n",
    "\n",
    "Our approach for chunking is pretty naive, but highlights the ease of getting at least something working for our application. We made some effort to keep the chunk size small so that our models are able to wield it effectively as context, but how are we going to reason about all of these pieces?\n",
    "\n",
    "**When extending and optimizing this approach for an arbitrary set of documents, some potential options include:**\n",
    "\n",
    "- Identifying logical breaks or synthesis techniques (manually, automatically, LLM-assisted, etc).\n",
    "- Aiming to construct chunks that are rich in unique and relevant information, avoiding redundancy to maximize database utility.\n",
    "- Customizing chunking to fit the document‚Äôs nature, ensuring the chunks are contextually relevant and cohesive.\n",
    "- Including key concepts, keywords, or metadata snippets in each chunk for improved searchability and relevance in the database.\n",
    "- Continuously assessing chunking effectiveness and be ready to adjust strategies for optimal balance between size and content richness.\n",
    "- Considering a hierarchy system (implicitly-generated or explicitly-specified) to improve retrieval attempts.\n",
    "    - If interested, please look over the [**LlamaIndex tree structures from the index guide**](https://docs.llamaindex.ai/en/stable/module_guides/indexing/index_guide.html#tree-index) as a starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-0QApYgNbyJD",
   "metadata": {
    "id": "-0QApYgNbyJD"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4: [Exercise]** Refining Summaries\n",
    "\n",
    "To automatically reason about large documents, one potential idea might be to use LLMs to create a dense summary or knowledge base. Similar to how we maintained a running history of the conversation via slot-filling in the previous notebook, is there any problem with keeping a running history of an entire document?\n",
    "\n",
    "In this section, we focus on an exciting application of LLMs: **automatically refining, coercing, and consolidating data en masse**. Specifically, we'll be implementing a simple but useful Runnable that uses a while loop and the running state chain formulation to summarize a set of document chunks. This process is commonly known as [**\"document refinement\"**](https://js.langchain.com/v0.1/docs/modules/chains/document/refine/) and is largely akin to our previous conversation-focused slot-filling exercise; the only difference is that now we're dealing with a large document instead of a growing chat history.\n",
    "\n",
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1J2XR8Cc8YSkVJMiJCknMkgA02mBT8riZ\" width=1000px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/doc_refine.png\" width=1000px/>\n",
    ">\n",
    "> From [**Refine | LangChain**ü¶úÔ∏èüîó](https://js.langchain.com/v0.1/docs/modules/chains/document/refine/)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **The DocumentSummaryBase Model**\n",
    "\n",
    "Much like the `KnowledgeBase` class from the previous notebook, we can create a `DocumentSummaryBase` structure designed to encapsulate the essence of a document. The one below will use the `running_summary` field to query the model for a final summary while attempting to use the `main_ideas` and `loose_ends` fields as a bottleneck to keep the running summary from moving too fast. This is something we're going to have to enforce via prompt engineering, so the `summary_prompt` is also provided which shows how this information will be used. Feel free to modify it as necessary to make it work for your model of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "gE8y2JvLvZ5T",
   "metadata": {
    "id": "gE8y2JvLvZ5T"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "class DocumentSummaryBase(BaseModel):\n",
    "    running_summary: str = Field(\"\", description=\"Running description of the document. Do not override; only update!\")\n",
    "    main_ideas: List[str] = Field([], description=\"Most important information from the document (max 3)\")\n",
    "    loose_ends: List[str] = Field([], description=\"Open questions that would be good to incorporate into summary, but that are yet unknown (max 3)\")\n",
    "\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are generating a running summary of the document. Make it readable by a technical user.\"\n",
    "    \" After this, the old knowledge base will be replaced by the new one. Make sure a reader can still understand everything.\"\n",
    "    \" Keep it short, but as dense and useful as possible! The information should flow from chunk to (loose ends or main ideas) to running_summary.\"\n",
    "    \" The updated knowledge base keep all of the information from running_summary here: {info_base}.\"\n",
    "    \"\\n\\n{format_instructions}. Follow the format precisely, including quotations and commas\"\n",
    "    \"\\n\\nWithout losing any of the info, update the knowledge base with the following: {input}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7LkjfpOAvlEd",
   "metadata": {
    "id": "7LkjfpOAvlEd"
   },
   "source": [
    "<br>\n",
    "\n",
    "We will also use this opportunity to bring back the `RExtract` function from the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "khRhVghHxBaz",
   "metadata": {
    "id": "khRhVghHxBaz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\['\n",
      "C:\\Users\\okafo\\AppData\\Local\\Temp\\ipykernel_20488\\187674538.py:14: SyntaxWarning: invalid escape sequence '\\]'\n",
      "  .replace(\"\\]\", \"]\")\n",
      "C:\\Users\\okafo\\AppData\\Local\\Temp\\ipykernel_20488\\187674538.py:15: SyntaxWarning: invalid escape sequence '\\['\n",
      "  .replace(\"\\[\", \"[\")\n"
     ]
    }
   ],
   "source": [
    "def RExtract(pydantic_class, llm, prompt):\n",
    "    '''\n",
    "    Runnable Extraction module\n",
    "    Returns a knowledge dictionary populated by slot-filling extraction\n",
    "    '''\n",
    "    parser = PydanticOutputParser(pydantic_object=pydantic_class)\n",
    "    instruct_merge = RunnableAssign({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
    "    def preparse(string):\n",
    "        if '{' not in string: string = '{' + string\n",
    "        if '}' not in string: string = string + '}'\n",
    "        string = (string\n",
    "            .replace(\"\\\\_\", \"_\")\n",
    "            .replace(\"\\n\", \" \")\n",
    "            .replace(\"\\]\", \"]\")\n",
    "            .replace(\"\\[\", \"[\")\n",
    "        )\n",
    "        # print(string)  ## Good for diagnostics\n",
    "        return string\n",
    "    return instruct_merge | prompt | llm | preparse | parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b638c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: langchain 1.0.8\n",
      "Uninstalling langchain-1.0.8:\n",
      "  Successfully uninstalled langchain-1.0.8\n",
      "Found existing installation: langchain-core 1.0.6\n",
      "Uninstalling langchain-core-1.0.6:\n",
      "  Successfully uninstalled langchain-core-1.0.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in c:\\programdata\\anaconda3\\lib\\site-packages (24.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 24.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "Successfully installed pip-25.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y langchain langchain-core\n",
    "%pip install --upgrade pip\n",
    "%pip install --upgrade langchain langchain-core langchain-community langchain-nvidia-ai-endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oFtME_s4PRoW",
   "metadata": {
    "id": "oFtME_s4PRoW"
   },
   "source": [
    "<br>\n",
    "\n",
    "With this in mind, the following code invokes the running state chain in a for-loop to iterate over your documents! The only modification necessary should be the `parse_chain` implementation, which should pass the the state through a properly-configured `RExtract` chain from the last notebook. After this, the system should work decently to maintain a running summary of the document (though some tweaking of the prompt may be required depending on the model used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6sODIfHUgz6m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79192,
     "status": "ok",
     "timestamp": 1703112894722,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "6sODIfHUgz6m",
    "outputId": "7b5aee70-078b-458e-d2a7-e8601b789fb1"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core.beta'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m## Take the first 10 document chunks and accumulate a DocumentSummaryBase\u001b[39;00m\n\u001b[0;32m     44\u001b[0m summarizer \u001b[38;5;241m=\u001b[39m RSummarizer(DocumentSummaryBase(), instruct_llm, summary_prompt, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 45\u001b[0m summary \u001b[38;5;241m=\u001b[39m summarizer\u001b[38;5;241m.\u001b[39minvoke(docs_split[:\u001b[38;5;241m15\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\runnables\\base.py:5024\u001b[0m, in \u001b[0;36minvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5016\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(\n\u001b[0;32m   5017\u001b[0m     input_: Input,\n\u001b[0;32m   5018\u001b[0m     run_manager: AsyncCallbackManagerForChainRun,\n\u001b[0;32m   5019\u001b[0m     config: RunnableConfig,\n\u001b[0;32m   5020\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   5021\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m   5022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m call_func_with_variable_args(\n\u001b[0;32m   5023\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, input_, config, run_manager\u001b[38;5;241m.\u001b[39mget_sync(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m-> 5024\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\runnables\\base.py:2089\u001b[0m, in \u001b[0;36m_call_with_config\u001b[1;34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   2086\u001b[0m config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m   2087\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m get_async_callback_manager_for_config(config)\n\u001b[0;32m   2088\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m-> 2089\u001b[0m     serialized,\n\u001b[0;32m   2090\u001b[0m     input_,\n\u001b[0;32m   2091\u001b[0m     run_type\u001b[38;5;241m=\u001b[39mrun_type,\n\u001b[0;32m   2092\u001b[0m     name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name(),\n\u001b[0;32m   2093\u001b[0m     run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   2094\u001b[0m )\n\u001b[0;32m   2095\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2096\u001b[0m     child_config \u001b[38;5;241m=\u001b[39m patch_config(config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\runnables\\config.py:430\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\runnables\\base.py:4881\u001b[0m, in \u001b[0;36m_invoke\u001b[1;34m(self, input_, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   4866\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   4867\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[0;32m   4868\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4871\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   4872\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m   4873\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this `Runnable` asynchronously.\u001b[39;00m\n\u001b[0;32m   4874\u001b[0m \n\u001b[0;32m   4875\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   4876\u001b[0m \u001b[38;5;124;03m        input: The input to this `Runnable`.\u001b[39;00m\n\u001b[0;32m   4877\u001b[0m \u001b[38;5;124;03m        config: The config to use.\u001b[39;00m\n\u001b[0;32m   4878\u001b[0m \u001b[38;5;124;03m        **kwargs: Additional keyword arguments.\u001b[39;00m\n\u001b[0;32m   4879\u001b[0m \n\u001b[0;32m   4880\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m-> 4881\u001b[0m \u001b[38;5;124;03m        The output of this `Runnable`.\u001b[39;00m\n\u001b[0;32m   4882\u001b[0m \n\u001b[0;32m   4883\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall_with_config(\n\u001b[0;32m   4885\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ainvoke,\n\u001b[0;32m   4886\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   4887\u001b[0m         ensure_config(config),\n\u001b[0;32m   4888\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4889\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\runnables\\config.py:430\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "Cell \u001b[1;32mIn[45], line 23\u001b[0m, in \u001b[0;36mRSummarizer.<locals>.summarize_docs\u001b[1;34m(docs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(docs):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m## TODO: Update the state as appropriate using your parse_chain component\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mpage_content\n\u001b[1;32m---> 23\u001b[0m     state \u001b[38;5;241m=\u001b[39m parse_chain\u001b[38;5;241m.\u001b[39minvoke(state)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo_base\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m state \n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\runnables\\passthrough.py:530\u001b[0m, in \u001b[0;36minvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)  \u001b[38;5;66;03m# noqa: TRY004\u001b[39;00m\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    521\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvalue,\n\u001b[0;32m    522\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapper\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m         ),\n\u001b[0;32m    527\u001b[0m     }\n\u001b[0;32m    529\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m--> 530\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;28minput\u001b[39m: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    533\u001b[0m     config: RunnableConfig \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    535\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall_with_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ainvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    540\u001b[0m     values: Iterator[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;66;03m# collect mapper keys\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\runnables\\base.py:2089\u001b[0m, in \u001b[0;36m_call_with_config\u001b[1;34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   2086\u001b[0m config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m   2087\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m get_async_callback_manager_for_config(config)\n\u001b[0;32m   2088\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m-> 2089\u001b[0m     serialized,\n\u001b[0;32m   2090\u001b[0m     input_,\n\u001b[0;32m   2091\u001b[0m     run_type\u001b[38;5;241m=\u001b[39mrun_type,\n\u001b[0;32m   2092\u001b[0m     name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name(),\n\u001b[0;32m   2093\u001b[0m     run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   2094\u001b[0m )\n\u001b[0;32m   2095\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2096\u001b[0m     child_config \u001b[38;5;241m=\u001b[39m patch_config(config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\runnables\\config.py:430\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\runnables\\passthrough.py:516\u001b[0m, in \u001b[0;36m_invoke\u001b[1;34m(self, value, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ainvoke\u001b[39m(\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    511\u001b[0m     value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    515\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 516\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    517\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)  \u001b[38;5;66;03m# noqa: TRY004\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\runnables\\base.py:4000\u001b[0m, in \u001b[0;36minvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3997\u001b[0m input_copies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(atee(inputs, \u001b[38;5;28mlen\u001b[39m(steps), lock\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39mLock()))\n\u001b[0;32m   3998\u001b[0m \u001b[38;5;66;03m# Create the transform() generator for each step\u001b[39;00m\n\u001b[0;32m   3999\u001b[0m named_generators \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m-> 4000\u001b[0m     (\n\u001b[0;32m   4001\u001b[0m         name,\n\u001b[0;32m   4002\u001b[0m         step\u001b[38;5;241m.\u001b[39matransform(\n\u001b[0;32m   4003\u001b[0m             input_copies\u001b[38;5;241m.\u001b[39mpop(),\n\u001b[0;32m   4004\u001b[0m             patch_config(\n\u001b[0;32m   4005\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4006\u001b[0m             ),\n\u001b[0;32m   4007\u001b[0m         ),\n\u001b[0;32m   4008\u001b[0m     )\n\u001b[0;32m   4009\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   4010\u001b[0m ]\n\u001b[0;32m   4012\u001b[0m \u001b[38;5;66;03m# Wrap in a coroutine to satisfy linter\u001b[39;00m\n\u001b[0;32m   4013\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_next_chunk\u001b[39m(generator: AsyncIterator) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\runnables\\base.py:3984\u001b[0m, in \u001b[0;36m_invoke_step\u001b[1;34m(step, input_, config, key)\u001b[0m\n\u001b[0;32m   3967\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   3968\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   3969\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3972\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   3973\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m   3974\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m   3975\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   3976\u001b[0m     )\n\u001b[0;32m   3978\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   3979\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[0;32m   3980\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3981\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   3982\u001b[0m     config: RunnableConfig \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3983\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m-> 3984\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m   3985\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config)\n\u001b[0;32m   3987\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_atransform\u001b[39m(\n\u001b[0;32m   3988\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3989\u001b[0m     inputs: AsyncIterator[Input],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3992\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[AddableDict]:\n\u001b[0;32m   3993\u001b[0m     \u001b[38;5;66;03m# Shallow copy steps to ignore mutations while in progress\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\runnables\\base.py:3218\u001b[0m, in \u001b[0;36minvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3205\u001b[0m run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3206\u001b[0m     cm\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m   3207\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3214\u001b[0m     )\n\u001b[0;32m   3215\u001b[0m ]\n\u001b[0;32m   3217\u001b[0m \u001b[38;5;66;03m# invoke\u001b[39;00m\n\u001b[1;32m-> 3218\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   3219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_exceptions:\n\u001b[0;32m   3220\u001b[0m         \u001b[38;5;66;03m# Track which inputs (by index) failed so far\u001b[39;00m\n\u001b[0;32m   3221\u001b[0m         \u001b[38;5;66;03m# If an input has failed it will be present in this map,\u001b[39;00m\n\u001b[0;32m   3222\u001b[0m         \u001b[38;5;66;03m# and the value will be the exception that was raised.\u001b[39;00m\n\u001b[0;32m   3223\u001b[0m         failed_inputs_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;167;01mException\u001b[39;00m] \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_core.beta'"
     ]
    }
   ],
   "source": [
    "latest_summary = \"\"\n",
    "\n",
    "## TODO: Use the techniques from the previous notebook to complete the exercise\n",
    "def RSummarizer(knowledge, llm, prompt, verbose=False):\n",
    "    '''\n",
    "    Exercise: Create a chain that summarizes\n",
    "    '''\n",
    "    ###########################################################################################\n",
    "    ## START TODO:\n",
    "\n",
    "    def summarize_docs(docs):        \n",
    "        ## TODO: Initialize the parse_chain appropriately; should include an RExtract instance.\n",
    "        ## HINT: You can get a class using the <object>.__class__ attribute...\n",
    "        parse_chain = RunnableAssign({'info_base' : RExtract(knowledge.__class__, llm, prompt)})\n",
    "        ## TODO: Initialize a valid starting state. Should be similar to notebook 4\n",
    "        state = {'info_base' : knowledge}\n",
    "\n",
    "        global latest_summary  ## If your loop crashes, you can check out the latest_summary\n",
    "        \n",
    "        for i, doc in enumerate(docs):\n",
    "            ## TODO: Update the state as appropriate using your parse_chain component\n",
    "            state['input'] = doc.page_content\n",
    "            state = parse_chain.invoke(state)\n",
    "\n",
    "            assert 'info_base' in state \n",
    "            if verbose:\n",
    "                print(f\"Considered {i+1} documents\")\n",
    "                pprint(state['info_base'])\n",
    "                latest_summary = state['info_base']\n",
    "                clear_output(wait=True)\n",
    "\n",
    "        return state['info_base']\n",
    "        \n",
    "    ## END TODO\n",
    "    ###########################################################################################\n",
    "    \n",
    "    return RunnableLambda(summarize_docs)\n",
    "\n",
    "# instruct_model = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\").bind(max_tokens=4096)\n",
    "instruct_model = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\").bind(max_tokens=4096)\n",
    "instruct_llm = instruct_model | StrOutputParser()\n",
    "\n",
    "## Take the first 10 document chunks and accumulate a DocumentSummaryBase\n",
    "summarizer = RSummarizer(DocumentSummaryBase(), instruct_llm, summary_prompt, verbose=True)\n",
    "summary = summarizer.invoke(docs_split[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07eb5710-23f7-4782-84eb-1fc8f73500b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"></span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(latest_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tKtoLf6DPv4Z",
   "metadata": {
    "id": "tKtoLf6DPv4Z"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5:** Synthetic Data Processing\n",
    "\n",
    "As we conclude our exploration of document summarization using LLMs, it's important to acknowledge the broader context and potential challenges. While we've demonstrated a viable method for extracting concise, meaningful summaries, let's consider why such an approach is crucial and the complexities it entails.\n",
    "\n",
    "#### **Generality of Refinement**\n",
    "\n",
    "It's important to note that this \"progressive summarization\" technique is merely a starter chain that makes few assumptions about the initial data and desired output format. The same technique can be expanded far and wide to generate synthetic refinements with known metadata, active assumptions, and downstream objectives in mind.\n",
    "\n",
    "**Consider these potential applications:**\n",
    "\n",
    "1. **Aggregating Data**: Constructing structures that transform raw data from document chunks into coherent, useful summaries.\n",
    "2. **Categorization and Sub-topic Analysis**: Creating systems that categorize insights from chunks into defined categories, tracking emerging sub-topics within each.\n",
    "3. **Consolidation into Dense Informational Chunks**: Refining these structures to distill insights into compact segments, enriched with direct quotes for deeper analysis.\n",
    "\n",
    "These applications hint at the creation of a **domain-specific knowledge graph** which can be accessed and traversed by a conversational chat model. Some utilities already exist to generate these automatically via tools like [**LangChain Knowledge Graphs**](https://python.langchain.com/docs/tutorials/graph/). Though you might need to develop hierarchical structures and tools to both construct and traverse such a structure, it is a viable option when you can properly refine a sufficient knowledge graph for your use case! For this intereted in more advanced knowledge graph construction techniques which rely on larger systems and vector similarity, we found the [**LangChain x Neo4j Article**](https://blog.langchain.dev/using-a-knowledge-graph-to-implement-a-devops-rag-application/) to be of interest.\n",
    "\n",
    "### **Addressing the Challenges of Large-Scale Data Processing**\n",
    "\n",
    "While our approach opens up exciting possibilities, it's not without its challenges, especially when dealing with large volumes of data:\n",
    "\n",
    "- **Generic Preprocessing Limitations**: While summarization is relatively straightforward, developing hierarchies that are universally effective across various contexts is challenging.\n",
    "\n",
    "- **Granularity and Navigation Costs**: Achieving detailed granularity in a hierarchy can be resource-intensive, requiring sophisticated consolidation or extensive branching to maintain manageable context sizes per interaction.\n",
    "\n",
    "- **Dependency on Precise Instruction Execution**: Navigating such a hierarchy with our current tools would rely heavily on powerful instruction-tuned models with strong prompt engineering. The inference latency and the risk of errors in argument prediction can be significant, so using LLMs for this could be a challenge.\n",
    "\n",
    "As you progress through the course, keep track of how these challenges get addressed with subsequent techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdFSMXOVRzEa",
   "metadata": {
    "id": "cdFSMXOVRzEa"
   },
   "source": [
    "-----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 6:** Wrap-Up\n",
    "\n",
    "The goal of this notebook was to introduce the problems and techniques surrounding large document handling for chat models. In the next notebook, we will investigate a complementary tool with a very different set of pros and cons; **semantic retrieval with embedding models.**\n",
    "\n",
    "### <font color=\"#76b900\">**Great Job!**</font>\n",
    "\n",
    "### **Next Steps:**\n",
    "1. **[Optional]** Revisit the **\"Questions To Think About\" Section** at the top of the notebook and think about some possible answers.\n",
    "2. **[Optional]** This notebooks includes some fundamental document processing chains, but does not touch upon [Map Reduce](https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain/) chains which are also very useful and build on roughly the same intuitions. These are a good next step, so please check them out!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8098de2f-32b3-428e-8f3b-f54141ec40b4",
   "metadata": {
    "id": "8098de2f-32b3-428e-8f3b-f54141ec40b4"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
